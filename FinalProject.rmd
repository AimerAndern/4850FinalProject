---
title: "SS4850 Final Project"
author: "Rui Zhu"
date: "4/6/2021"
output:
  pdf_document: default
  word_document: default
---



# Explanatory analysis

```{r}
HCVdata <- read.csv(file = 'hcvdat0.csv')
suppressMessages(library(dplyr))
drop <- c("X")
HCVdata = HCVdata[,!(names(HCVdata) %in% drop)]
colnames(HCVdata) = c("Category","Age","Sex","ALB","ALP","ALT","AST",
                      "BIL","CHE","CHOL","CREA","GGT","PROT")

# 1 and 2 for blood donor and suspect blood donor, 2,3,4 for hepatitis, fibrosis,cirrhosis respectively
HCVdata$Category <- factor(HCVdata$Category,labels=c(1,2,3,4,5),
                           levels=c('0=Blood Donor',
                                    '0s=suspect Blood Donor',
                                    '1=Hepatitis',
                                    '2=Fibrosis', 
                                    '3=Cirrhosis'))


# 0 for male, 1 for female
HCVdata$Sex <- factor(HCVdata$Sex,levels=c("m","f"),labels=c("0","1"))
HCVdata=na.omit(HCVdata[c("Category","Age","Sex","ALB","ALP","ALT","AST",
                          "BIL","CHE","CHOL","CREA","GGT","PROT")])
for(var in 1:13)
{
  HCVdata[,var]=as.numeric(HCVdata[,var])
}

tail(HCVdata)
```

# EDA!!!

```{r}
#class information, distribution of "category"
library(dplyr)
HCVdata %>% 
  group_by(Category) %>%
  summarise(no_rows = length(Category))
#mean of each variable
colMeans(HCVdata)
#median of each variable
apply(HCVdata,2,median)
```


## train and test split 7:3
```{r}
## train and test set 7:3
HCVdata<-HCVdata[complete.cases(HCVdata),]
set.seed(4850)
sample <- sample.int(n = nrow(HCVdata), size = floor(.70*nrow(HCVdata)), replace = F)
train <- HCVdata[sample, ]
test <- HCVdata[-sample, ]

```

## roc with random forest

```{r}
#binary category
#1 for HCV, 0 for donor(regular/suspect)
set.seed(4850)
train$binary <- ifelse(as.numeric(train$Category)>2,1,0)
test$binary <- ifelse(as.numeric(test$Category)>2,1,0)
#barplot on binary
counts <- table(train$binary)
barplot(counts, main="HCV distribution",
   xlab="Binary")
#load libraries
library(randomForest)
rfmod <- randomForest(formula = binary ~ .-Category, data = train, ntree = 10, maxnodes= 100, norm.votes = F) 
suppressMessages(library(dplyr))
testm <- mutate(test,
                rf_pred = predict(rfmod,newdata = test,
                                   type="response"))

suppressMessages(library(pROC))

roc1 <- roc(testm$binary,testm$rf_pred)
AUC1 <- auc(roc1)
rs <- roc1[['rocs']]


```

## Mixed model GLMM

### binary random: AST

```{r}
suppressMessages(library(ggpubr))
library(ggplot2)
agePlot=ggplot(train, aes(x=Age,y=binary))+geom_point()
ALPPlot=ggplot(train, aes(x=ALP,y=binary))+geom_point()
ALBPlot=ggplot(train, aes(x=ALB,y=binary))+geom_point()

# regular logistic model
bmod <- glm(binary ~.-Category,binomial,data=,train)
summary(bmod)

#GLMM model random effect:Age
library(MASS)
set.seed(4850)
suppressMessages(modpql <- glmmPQL(binary ~Sex+ALB+Age+ALT+ALT+BIL+CHE+CHOL+CREA+GGT+PROT,
                  data=train, random= ~ 1|AST, family=binomial))

modpql$rsquared
typeof(modpql)
suppressMessages(library(dplyr))
testm <- mutate(test,
                glmm_pred = predict(modpql,newdata = test,
                                   type="response"))

suppressMessages(library(pROC))

roc2 <- roc(testm$binary,testm$glmm_pred)
AUC <- auc(roc2)
rs <- roc2[['rocs']]
```

# Random effect:ALT
```{r}
#GLMM model random effect:ALT
library(MASS)
set.seed(4850)
suppressMessages(modpql <- glmmPQL(binary ~Sex+ALB+Age+ALP+AST+BIL+CHE+CHOL+CREA+GGT+PROT,
                  data=train, random= ~ 1|ALT, family=binomial))

modpql$rsquared
typeof(modpql)
suppressMessages(library(dplyr))
testm <- mutate(test,
                glmm_pred = predict(modpql,newdata = test,
                                   type="response"))

suppressMessages(library(pROC))

roc22 <- roc(testm$binary,testm$glmm_pred)
AUC <- auc(roc22)
rs <- roc22[['rocs']]
```

# Random effect:ALP
```{r}
#GLMM model random effect:ALT
library(MASS)
set.seed(4850)
suppressMessages(modpql <- glmmPQL(binary ~Sex+ALB+Age+ALP+AST+BIL+CHE+CHOL+CREA+GGT+PROT,
                  data=train, random= ~ 1|ALP, family=binomial))
suppressMessages(drop1(modpql, test="Chi"))


summary(modpql)
modpql$rsquared
typeof(modpql)
suppressMessages(library(dplyr))
testm <- mutate(test,
                glmm_pred = predict(modpql,newdata = test,
                                   type="response"))

suppressMessages(library(pROC))

roc23 <- roc(testm$binary,testm$glmm_pred)
AUC <- auc(roc23)
rs <- roc23[['rocs']]
coords(roc23,"best")
```



## Deep learning

### binary

```{r}
library(dplyr)
library(keras)
library(tensorflow)
library(tfdatasets)



set.seed(4850)
# Store the overall correlation in `M`
M <- cor(train[,1:10])

# Plot the correlation plot with `M`
library(corrplot)
predictorsCorr=corrplot(M, method="circle")

figure <- ggarrange(agePlot, ALPPlot, ALBPlot,predictorsCorr,
                    labels = c("A", "B", "C","D"),
                    ncol = 2, nrow = 2)
figure
#normalize 
spec <- feature_spec(train, binary ~ .-Category ) %>% 
  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>% 
  fit()
spec

layer <- layer_dense_features(
  feature_columns = dense_features(spec), 
  dtype = tf$float32
)
suppressMessages(layer(train))

input <- layer_input_from_dataset(train[,2:13])


output <- input %>% 
  layer_dense_features(dense_features(spec)) %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1) 

dpmod <- keras_model(input, output)





dpmod %>% 
  compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )

build_model <- function() {
  input <- layer_input_from_dataset(train[,2:13])
  
  output <- input %>% 
    layer_dense_features(dense_features(spec)) %>% 
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1) 
  
  dpmod <- keras_model(input, output)
  
  dpmod %>% 
    compile(
      loss = "mse",
      optimizer = optimizer_rmsprop(),
      metrics = list("mean_absolute_error")
    )
  
  dpmod
}


# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)    

dpmod <- build_model()

history <- dpmod %>% fit(
  x = train[2:13],
  y = train$binary,
  epochs = 500,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(print_dot_callback)
)


library(ggplot2)

plot(history)



# it will stop when no more improvement
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

dpmod <- build_model()

history <- dpmod %>% fit(
  x = train[2:13],
  y = train$binary,
  epochs = 500,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(early_stop)
)

plot(history)

suppressMessages(library(dplyr))
testm <- mutate(test,
                dp_pred = dpmod %>% predict(test[,2:13]), type="response")



roc3 <- roc(testm$binary,testm$dp_pred)
AUC3 <- auc(roc3)
rs <- roc3[['rocs']]
suppressMessages(summary(dpmod))
```


```{r}
#RM,(AST ALT ALP) glmm, nn
coords(roc1,"best",ret=c("threshold",
"specificity", "sensitivity", "accuracy","precision"))##the random forest model

coords(roc2,"best",ret=c("threshold",
"specificity", "sensitivity", "accuracy","precision"))# the AST GLMM model
coords(roc22,"best",ret=c("threshold",
"specificity", "sensitivity", "accuracy","precision"))#the ALT GLMM model
coords(roc23,"best",ret=c("threshold",
"specificity", "sensitivity", "accuracy","precision"))# the ALP GLMM model
coords(roc3,"best",ret=c("threshold",
"specificity", "sensitivity", "accuracy","precision"))# the nn GLMM model

#importance: impurity gini importance
importance(rfmod)
#neural network and random forest
plot(roc1)
plot(roc3, add=TRUE, col='blue')
legend(1.45, 0.5, legend=c("RF", "NN"),
       col=c( "black","blue"), lty=1, cex=0.8)


#three glmm roc curve
plot(roc2)
plot(roc22, add=TRUE, col='red')
plot(roc23, add=TRUE, col='blue')
legend(1.45, 0.5, legend=c("AST", "ALT", "ALP"),
       col=c( "black","red", "blue"), lty=1, cex=0.8)
#randomforest best glmm and cnn roc curve
plot(roc1)
plot(roc23, add=TRUE, col='red')
plot(roc3, add=TRUE, col='blue')
legend(1.45, 0.5, legend=c("RF", "ALP-GLMM", "NN"),
       col=c( "black","red", "blue"), lty=1, cex=0.8)
```



































